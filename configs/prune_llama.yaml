_base_ : ['./_base_/c4.yaml']

model:
    model_class: "LlamaForCausalLM"
    model_name: "meta-llama/Llama-2-7b-hf"
    dtype: float
    tokenizer_class: "AutoTokenizer"
    tokenizer_name: "meta-llama/Llama-2-7b-hf"

pruning_dir: "./workdirs/prune_llama/"

pruner:
    type: ForwardPruner
    dual_insert_layer: model.embed_tokens
    criterion:
        scope: "global"
        strategy: "forward_grads"
        exclude_layers: [ "embed_tokens", "norm", "lm_head", "self_attn" ]
    sparsities: [ 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4 ]

test_cfg:
    # testing time sparsity can be a subset of pruning time sparsity
    sparsities: [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]
    use_prior: False
    print_table: False
    testing_manager:
        type: ForwardPrunerTestingManager
    evaluator:
        type: Perplexity
