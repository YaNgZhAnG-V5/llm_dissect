_base_: ["./_base_/datasets.yaml"]

pruning_dataset: {{ _base_.perplexity }}
recons_dataset: {{ _base_.c4 }}
test_dataset: {{ _base_.wikitext }}

model:
    model_class: "LlamaForCausalLM"
    model_name: "meta-llama/Meta-Llama-3-70B"
    model_args:
        torch_dtype: bfloat16
        do_sample: False
        use_cache: True
    manual_dispatch: True
    dispatch_cfg:
        # TODO: remove user name. It is very dangerous to use this.
        checkpoint: "{{$HF_HOME:~/.cache/huggingface}}/hub/models--meta-llama--Meta-Llama-3-70B/snapshots/b33784c5adf6e4b1a60d041da74e83fd438d67cd"
        offload_folder: offload
        offload_state_dict: True
        no_split_module_classes:
          - "LlamaDecoderLayer"
    tokenizer_class: "AutoTokenizer"
    tokenizer_name: "meta-llama/Meta-Llama-3-70B"
    mem_efficient_sdp: True

test_cfg:
    eval_original: True
    # testing time sparsity can be a subset of pruning time sparsity
    sparsities: [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6]
    use_prior: False
    print_table: False
    testing_manager:
        type: ForwardPrunerTestingManager
        in_place: False # in_place has to be False
        prune_input:
            - NoItem
    evaluator:
        type: Output
        distance_metric: js_divergence
    runtime_evaluator:
        type: InferenceTime
        num_repetitions: 10
        warmup_steps: 0
    second_evaluator:
        type: LMEvalHarness
        lm_eval_cfg:
            tasks: ['openbookqa', 'piqa', 'hellaswag', 'winogrande', 'arc_easy', 'arc_challenge', 'boolq', 'lambada']
            batch_size: 2
            random_seed: 42
            num_fewshot: 0
            # number of maximal samples used in each tasks
            limit: 1000
        lm_wrapper_cfg: {}
#    macs_evaluator:
#        type: MacsEvaluator
#        seq_len: 8192
