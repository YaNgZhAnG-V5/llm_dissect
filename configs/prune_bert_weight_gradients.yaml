dataset:
    dataset_name: 'imbd'
    split: train
    num_samples: 3000
    max_length: 512
    use_label: False

data_loader:
    batch_size: 16
    shuffle: False

model:
    model_class: "AutoModelForSequenceClassification"
    # model_name is fed into .from_pretrained function. Fine-tuned ckpt already contains model configuration
    model_name: "./workdirs/bert-imdb-finetuned/checkpoint-188"
    dtype: float
    tokenizer_class: "AutoTokenizer"
    tokenizer_name: "bert-base-uncased"

pruner:
    type: WeightGradientsPruner
    criterion:
        scope: "global"
        strategy: "weight_grads"
        exclude_layers: ["embeddings", "LayerNorm", "classifier"]
    sparsities: [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]

test_cfg:
    # testing time sparsity can be a subset of pruning time sparsity
    sparsities: [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]
    print_table: True
    use_prior: False
    testing_manager:
        type: WeightGradientsTestingManager
    evaluator:
        type: Accuracy
